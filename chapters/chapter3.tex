\chapter{METHODOLOGY}

This chapter comprehensively describes the methodology employed in this study, covering the [proposed approach], [technique 1], [technique 2], and the [main algorithm] implementation. The proposed method enhances [baseline system] for [target application] by employing efficient [processing methods] that reduce [constraint metric] while maintaining high [performance metric].

\section{Overview of Proposed Method}

Our method enhances [baseline system] for [target application] through three key aspects: (1) adopting [technique 1] for [process 1], (2) evaluating the effect of [parameter optimization], and (3) modifying [baseline system] and using it as a baseline model for comparative analysis.

The workflow follows a systematic approach to preprocess [data type], apply advanced [processing techniques], and train [model type] models. The process begins with [data preparation step] to generate manageable data [units]. These [units] are preprocessed using [technique] to [goal] effectively, after which they are used to train [model type] for [task type] tasks.

Key components of the workflow include:

\begin{enumerate}
\item \textbf{Data Preprocessing:} [Process 1], [process 2], and [process 3] using [number] different methods
\item \textbf{Model Design and Training:} Development of [model architecture] for [task] tasks
\item \textbf{Model Optimization:} [Parameter] reduction to enable deployment on [target environment]
\item \textbf{Evaluation:} Assessment of model performance using [evaluation methods] and appropriate metrics
\end{enumerate}

\section{Dataset Preparation}

We partitioned the selected [dataset name] into two groups: [group 1] for testing and [group 2] for training/validation ([ratio] split). The dataset split ensures clean evaluation of the model's performance by maintaining separation between training and testing data sources.

\begin{table}[htbp]
\caption{Class Distribution Across Dataset Splits Before Augmentation}
\label{tab:class_distribution}
% For screenshot style, replace with: \includegraphics[width=\textwidth]{images/table_class_distribution.png}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Dataset Split} & \textbf{[Class 1]} & \textbf{[Class 2]} & \textbf{Ratio} \\
& \textbf{(Class 0)} & \textbf{(Classes 1–N)} & \textbf{([Class 1]:[Class 2])} \\
\hline
Training & [Number 1] & [Number 2] & [Ratio 1] \\
Validation & [Number 3] & [Number 4] & [Ratio 2] \\
Test & [Number 5] & [Number 6] & [Ratio 3] \\
\hline
\end{tabular}
\end{table}

To address the class imbalance shown in Table \ref{tab:class_distribution}, data augmentation was applied exclusively to the training dataset, increasing "[class 1]" samples to [target number] and achieving a balanced 1:1 ratio. Custom augmentation methods introduced diversity given the dataset's limited variability. The augmentation techniques included:

\begin{enumerate}
\item \textbf{[Augmentation 1]} (applied with probability p = [value]) to simulate [variation type]
\item \textbf{[Augmentation 2]} ([parameter range], p = [value]) to mimic [variation type]
\end{enumerate}

For each sample, [number] to [number] augmentations are randomly selected and applied sequentially, ensuring diversity without over-altering the [data characteristics].

\section{[Primary Processing] Methods}

[Primary processing] isolates potential [target objects] from the [background]. We implemented and compared [number] approaches, each with distinct computational characteristics.

\subsection{[Method 1] vs. [Method 2]}

In the original [baseline system] \cite{sample2022reference}, a [processing result] for each [data unit] was created as a [mathematical operation] of the previous [number] [data units]. While effective for [goal], this method requires significant computation. Our proposed [method 2] substantially reduces computational demands through an exponential weighting function:

\begin{equation}
M_t = (1 - \lambda)M_{t-1} + \lambda D_t
\label{eq:method_equation}
\end{equation}

where $M_t$ is the [model] at time t, $D_t$ is the current [data unit] at time t, and $\lambda$ is the updating rate which determines the rate at which [changes] are incorporated into the [model]. An excessively large $\lambda$ can lead to [negative effect] \cite{sample2010reference}. Here we chose to use $\lambda = [value]$ to control how quickly the model adapts to changes.

The advantages of using [method 2] are threefold:
\begin{enumerate}
\item \textbf{Memory efficiency} by only storing the previous [model]
\item \textbf{Increased computational speed} by removing [expensive operation]
\item \textbf{Adaptive response} via a tunable learning rate parameter that adjusts the [adaptation speed] to suit [environmental conditions]
\end{enumerate}

The [output mask] is generated by thresholding the absolute difference between current [data unit] and [model]:

\begin{equation}
O_t = |D_t - M_t| > T
\label{eq:output_mask}
\end{equation}

where $O_t$ is the [output mask] at time t, and T = [threshold value] is our empirical threshold that distinguishes [target] from [background]. We refine this mask through a sequence of morphological operations ([operation 1], [operation 2], [operation 3]) using a [kernel size] kernel, eliminating small components (< [size] pixels), and applying a custom [filter type] filter.

\subsection{[Additional Technique] Integration}

We integrated [additional technique] analysis to distinguish between [noise type] and [target objects]. Using [algorithm name] \cite{sample2003reference} with optimised parameters, we identify and exclude regions with characteristic [unwanted patterns] from the [output mask]. This approach significantly reduces [error type] while maintaining sensitivity to actual [targets], enabling more reliable detection in environments with [challenging conditions].

\subsection{Custom [Algorithm] Model}

To provide a baseline comparison for our [Method 2], we implemented a Custom [Algorithm] Model (C[Algorithm]) based on the adaptive approach by [Author] \cite{sample2004reference, sample2006reference}. Our C[Algorithm] models use a mixture of K = [number] [distributions] to track the temporal evolution of each [data element].

Our C[Algorithm] implementation differs from standard [standard algorithm] in [number] key aspects:
\begin{enumerate}
\item Using [distance metric] instead of [standard metric] for matching
\item [Modification 1] with a single active component
\item Simplified [model component] using only the [selection criteria]
\item Adaptive [component management]
\item Outputting [output type] instead of [standard output type]
\end{enumerate}

The C[Algorithm] operates through the following steps:

\begin{algorithm}[!ht]
\caption{Simplified [Algorithm] Model [Processing]}
\label{alg:custom_algorithm}
\begin{algorithmic}[1]
\REQUIRE Current [data unit] $D_t$, learning rate $\alpha=[value]$
\ENSURE [Output] $O_t$
\IF{first [data unit]}
    \STATE Initialize [model] $\mu \leftarrow D_t$
    \STATE $O_t \leftarrow$ zero matrix
\ELSE
    \STATE Calculate difference: $\delta \leftarrow D_t - \mu$
    \STATE Update [model]: $\mu \leftarrow \mu + \alpha \cdot \delta$
    \STATE Generate [output]: $O_t \leftarrow |D_t - \mu|$
\ENDIF
\RETURN $O_t$
\end{algorithmic}
\end{algorithm}

\section{[Parameter] Scaling}

To further reduce computational costs, we investigated the effect of input [parameter] on classification accuracy and processing speed. The original [baseline system] processes [data] at [original dimensions] with [rate] ([unit]), creating significant computational costs.

Our experiments evaluated [number] [parameter] sizes using [constant parameter]:
\begin{enumerate}
\item \textbf{Original size:} [dimension 1] (baseline)
\item \textbf{Reduced size:} [dimension 2] ([percentage]% fewer [units])
\item \textbf{Minimal size:} [dimension 3] ([percentage]% fewer [units])
\end{enumerate}

Each size shares the same [constant dimension], while reducing [variable dimensions]. [Scaling method] was performed using [interpolation method] to preserve important features while minimising computational requirements. The [model architecture] automatically scales its [feature components] through the [processing layers] while preserving the [important dimension] to effectively capture the [key characteristics].

This reduction in [parameter] substantially decreases the number of operations required for both [process 1] and [process 2], along with a corresponding reduction in memory usage, making the system suitable for [deployment target].

\section{[Model] Architecture}

Since our primary objective is improving the [preprocessing techniques] rather than the [model] itself, we reused the [baseline architecture] \cite{sample2022reference}. However, we implemented specific modifications to optimise it for our [task type] task.

\subsection{Network Architecture}

The [model] architecture consists of the following components:

\begin{enumerate}
\item \textbf{Input Layer:} The network accepts input [data] of dimension [dimensions] ([dimension descriptions]), where [variables] vary according to the selected [parameter].

\item \textbf{[Processing] Blocks:} The backbone consists of [number] [processing]-[operation] blocks:
    \begin{itemize}
    \item Each block contains: [layer type] → [normalization] → [pooling] → [regularization]
    \item The [number] [layer type] use [filter numbers] filters respectively
    \end{itemize}

\item \textbf{Fully Connected Layers:} Following the [processing] blocks, [number] dense layers with [units 1] and [units 2] units respectively, each followed by [regularization]

\item \textbf{Output Layer:} A final dense layer with [output units] units and [activation function] for [task type]
\end{enumerate}

\subsection{Training Configuration}

We adjusted the model training process by:
\begin{enumerate}
\item Replacing the original loss function with [loss function]
\item Modifying [regularization] rates to prevent overfitting
\item Adjusting [operation] parameters to better capture [target characteristics]
\end{enumerate}

The model consists of [parameter count] trainable parameters ([size] MB) and efficiently processes [feature type 1] and [feature type 2] features through its [processing] layers. We use the adapted architecture as a consistent baseline across all [preprocessing] configurations to enable fair comparison of our proposed optimisations.

\section{Implementation Details}

All experiments were conducted on a system with [OS details], [CPU details], [memory] GB RAM, and [GPU details]. The model was executed using [framework] and trained with the [optimizer].

The implementation workflow focuses on [process 1], [process 2], and [process 3]:

\begin{enumerate}
\item \textbf{[Process 1]:} [Data preparation] and application of [processing techniques] ([Method 1], [Method 2], and [Method 3])
\item \textbf{[Process 2]:} [Model] optimization using [optimizer] with [training techniques]
\item \textbf{[Process 3]:} Performance assessment using [metric 1], [metric 2], [metric 3], [metric 4], and [metric 5] measurements
\end{enumerate}

Comparative analyses are conducted to assess the impact of [processing methods] on model performance, providing insights into their suitability for different [conditions] and [constraints].